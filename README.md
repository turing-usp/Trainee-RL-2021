<img src="img/title.png" width="100%" />

Boas vindas ao Projeto Trainee 2021 da Ã¡rea de Aprendizado por ReforÃ§o!

Antes de mais nada, recomendamos ler a seÃ§Ã£o de **[ğŸ‘©â€ğŸ« IntroduÃ§Ã£o](https://github.com/turing-usp/Aprendizado-por-Reforco/tree/main/Introdu%C3%A7%C3%A3o)** do nosso RepositÃ³rio de Aprendizado por ReforÃ§o para se familiarizar com os conceitos principais da Ã¡rea!

O projeto serÃ¡ dividido em duas partes:

<img src="img/tarefa1.png" width="100%" />

Nessa primeira etapa do projeto, vocÃª construirÃ¡ um agente simples de Aprendizado por ReforÃ§o para entender um pouco melhor os principais conceitos da Ã¡rea aplicados em algum tipo de cÃ³digo. Aqui, vocÃª aprenderÃ¡ sobre o clÃ¡ssico problema dos *k*-Armed Bandits, como conseguir estimar valores para determinada aÃ§Ãµes com base na recompensa, como selecionar aÃ§Ãµes com esse maior valor estimado e como conseguir explorar o ambiente para que o agente descubra novas aÃ§Ãµes. VocÃª tambÃ©m aprenderÃ¡ como treinar esse agentes criados.

Para isso, recomendamos nosso **[ğŸ“° Turing Talks](https://medium.com/turing-talks/sua-primeira-ia-o-problema-dos-k-armed-bandits-cc63732567b2)** sobre o assunto e tambÃ©m a nossa implementaÃ§Ã£o dele no nosso **[ğŸ° RepositÃ³rio](https://github.com/turing-usp/Aprendizado-por-Reforco/tree/main/Aprendizado%20por%20Refor%C3%A7o%20Cl%C3%A1ssico/Bandits)**.

Comece agora mesmo acessando o notebook da [Tarefa 1 - Bandits](Tarefa%201%20-%20Bandits.ipynb)!

<img src="img/tarefa2.png" width="100%" />

Na segunda parte do projeto, vocÃª deverÃ¡ implementar e comparar diferentes algoritmos de Aprendizado por ReforÃ§o Profundo utilizando a biblioteca [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/).

Este repositÃ³rio jÃ¡ contÃ©m um tutorial simples de como utilizar a biblioteca, que vocÃª pode conferir **[aqui](Tutorial%20-%20Stable%20Baselines.ipynb)**!

Para comeÃ§ar a tarefa, basta acessar e alterar **[este notebook](Tarefa%202%20-%20Stable%20Baselines.ipynb)**!

<img src="img/tarefa_extra.png" width="100%" />

Por fim, propomos uma tarefa extra 100% opcional caso queira treinar suas habilidades de RL! Ao final do projeto Trainee, caso ache interessante, vocÃª pode optar por treinar um novo modelo para o ambiente **slime-volleyball**, um jogo de vÃ´lei no qual seu agente pode competir com os jogadores de outros membros.

Para treinar seu prÃ³prio agente, basta rodar [este Jupyter Notebook](https://colab.research.google.com/github/turing-usp/Trainee-RL-2021/blob/main/Tarefa%20Extra%20-%20SlimeVolleyball.ipynb) em seu Google Colaboratory. Recomendamos, antes de comeÃ§ar, que vocÃª converse com algum dos veteranos da Ã¡rea, copie o notebook para seu drive e edite os hiperparÃ¢metros conforme necessÃ¡rio!

**DISCLAMER:** Os agentes de vÃ´lei demorarÃ£o muitas horas (3+) para serem treinados e nem sempre superarÃ£o a IA jÃ¡ programada no prÃ³prio jogo, entÃ£o nÃ£o se desanime caso nÃ£o esteja obtendo resultados rapidamente! Lembrando que esta Ã© uma tarefa **100% opcional**.