{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/3U3hI1u.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boas vindas a sua segunda e última tarefa de Aprendizado por Reforço!\n",
    "\n",
    "Neste exercício, você deverá implementar e comparar diferentes algoritmos de **Aprendizado por Reforço Profundo** utilizando a biblioteca _[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)_.\n",
    "\n",
    "A _Stable Baselines_ é uma biblioteca de Aprendizado por Reforço que implementa diversos algoritmos de agentes, além de várias funcionalidades úteis para seu treinamento. Suas implementações são bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de reforço de alta qualidade.\n",
    "\n",
    "Antes de começar a tarefa, é importante acessar e se familiarizar com o tutorial da biblioteca disponível neste repositório! Depois de rodar o guia, você já estará capaz de completar este trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do Ambiente\n",
    "\n",
    "Antes de analisar o possíveis algoritmos, o primeiro passo é escolher qual ambiente você quer resolver! Para esta tarefa, separamos dois possíveis ambientes diferentes, em ordem de dificuldade, que você poderá escolher: **CartPole** e **Pendulum**. Lembrando que, quanto mais difícil um ambiente, mais demorado será o treinamento.\n",
    "\n",
    "A seguir, estão as descrições de cada um deles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">CartPole</h2>\n",
    "<img src=\"https://bytepawn.com/images/cartpole.gif\" width=50% />\n",
    "\n",
    "**CartPole** é o ambiente de Aprendizado por Reforço mais comum do Gym, no qual deve-se balancear um pêndulo invertido conectado a um carrinho, somente controlando os movimentos do carrinho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do CartPole é definido por 4 informações:\n",
    "\n",
    "<br>\n",
    "\n",
    "|     | Informação                         | Min     | Max    |\n",
    "| :-- | :--------------------------------- | :-----: | :----: |\n",
    "| 0   | Posição do Carrinho                | -4.8    | 4.8    |\n",
    "| 1   | Velocidade do Carrinho             | -Inf    | Inf    |\n",
    "| 2   | Ângulo da Barra                    | -24 deg | 24 deg |\n",
    "| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |\n",
    "\n",
    "<br>\n",
    "\n",
    "A posição do carrinho vai de -4.8 a 4.8, mas ele perde o episódio caso saia dos limites de -2.4 e 2.4. Da mesma forma, o ângulo da barra vai de -24° a 24°, porém o episódio acaba caso a barra saia dos limites de -12° e 12°.\n",
    "\n",
    "Já o **Espaço de Ação** é composto por duas ações únicas: mover o carrinho para a **esquerda** ou para a **direita**.\n",
    "\n",
    "Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos movê-lo para a direita, enviamos um `env.step(1)`\n",
    "\n",
    "| Ação | Significado           |\n",
    "| :--- | :-------------------- |\n",
    "| 0    | Mover para a esquerda |\n",
    "| 1    | Mover para a direito  |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, que é igual a +1 para cada instante que passa sem o agente perder. Assim, o CartPole é incentivado a sobreviver por mais tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o modelo, vamos precisar de 2 bibliotecas: **gym** (para inicialização dos ambientes) e **stable_baselines3** (para inicialização e avaliação dos modelos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Definindo ambiente\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Definindo modelo\n",
    "model = PPO(\"MlpPolicy\", env, seed=1, verbose=1)\n",
    "\n",
    "# Avaliando o agente\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa Média: {mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Pendulum</h2>\n",
    "\n",
    "<img src=\"https://www.gymlibrary.ml/_static/videos/classic_control/pendulum.gif\" width=30% />\n",
    "\n",
    "**Pendulum** é um ambiente do Gym que simula um pêndulo pendurado por um ponto tentando se balancear de cabeça para baixo. O agente deve um torque no pêndulo de forma que ele se levante e fique parado em pé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 8 informações.\n",
    "\n",
    "| Estado    | Informação                                     |\n",
    "| :-------- | :--------------------------------------------- |\n",
    "| 0         | Posição no eixo _x_ da ponta do pêndulo        |\n",
    "| 1         | Posição no eixo _y_ da ponta do pêndulo        |\n",
    "| 2         | Velocidade angular do pêndulo                  |\n",
    "\n",
    "Já o **Espaço de Ação** é um espaço **contínuo** do torque aplicado no pêndulo.\n",
    "\n",
    "| Ação | Significado     | Intervalo   |\n",
    "| :--- | :-------------- | :---------- |\n",
    "| 0    | Torque          | $-2$ a $+2$ |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, que segue a seguinte equação:\n",
    "\n",
    "$$r = -(\\theta{}^2 + 0.1 * \\dot{\\theta}^2 + 0.001 * \\tau{})$$\n",
    "\n",
    "Desta forma, temos que a recompensa é menor quando o ângulo do pêndulo é menor (mais em pé), quando a velocidade angular é baixa, e quando usamos pouco torque.\n",
    "\n",
    "Por ser um ambiente com espaço de ação contínuo, os algoritmos que podemos usar serão diferentes, e o treinamento pode ser mais demorado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente\n",
    "\n",
    "Para criar o ambiente, basta rodar a linha de código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Testando Modelos\n",
    "\n",
    "Caro piloto, agora que você conhece esses dois ambientes, é hora de brincar com eles. Você deverá testar diferentes algoritmos (a seu critério), e ver sua recompensa média. Para ver quais as limitações dos modelos, veja esse [link](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html). Abaixo, criamos uma função que será útil para comparar os modelos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(model, env, n_episodes, info_dict):\n",
    "    model_name = str(model.__class__).split(\".\")[-1][:-2]\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes, deterministic=True)\n",
    "    \n",
    "    info_dict[model_name] = {}\n",
    "    info_dict[model_name][\"mean_reward\"] = mean_reward\n",
    "    info_dict[model_name][\"std_reward\"] = std_reward\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "algorithms_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e Avaliando seu próprio modelo\n",
    "\n",
    "Primeiramente, agora você deve decidir em qual ambiente você deseja treinar seu agente. Para isto, basta tirar o comentário da linha referente ao ambiente escolhido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"CartPole-v1\"\n",
    "# env_name = \"Pendulum-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, você está livre para testar diferentes algoritmos para seu ambiente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import ... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo o ambiente\n",
    "env = gym.make(env_name)\n",
    "\n",
    "model = ... # Defina o modelo\n",
    "model.learn(total_timesteps=...) # Treine o modelo\n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente e guardando o desempenho no dicionário\n",
    "algorithms_dict = getValues(model, env, n_episodes, algorithms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço livre para testagem de diferentes algoritmos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, detalhe um pouco mais quais foram os algoritmos testados bem como a performance obtida por cada um.\n",
    "\n",
    "Este detalhamento pode ser feito por meio de um ou mais gráficos mostrando o desempenho dos modelos, ou simplesmente por texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço para o Piloto criar gráficos ou textos para mostrar os diferentes resultados entre modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do Algoritmo\n",
    "\n",
    "Após testar e analisar diversos algoritmos diferentes, qual foi o escolhido?\n",
    "\n",
    "_Pergunta Extra:_ você usou algum critério para escolher quais algoritmos seriam testados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Qual foi o algoritmo escolhido?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7f18c8a75fd108ead44a979ee6b583a0b7ac14f61cb5b6828c4aa8a7a81d42e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
