{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/3U3hI1u.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> descrever aqui em que consiste a tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do Ambiente\n",
    "\n",
    "Antes de analisar o possíveis algoritmos, o primeiro passo é escolher qual ambiente você quer resolver! Para esta tarefa, separamos quatro possíveis ambientes diferentes, em ordem de dificuldade, que você poderá escolher: **Pong**, **Lunar Lander** e **Lunar Lander Continuous**.\n",
    "\n",
    "A seguir, estão as descrições de cada um deles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Pong</h2>\n",
    "<img src=\"https://imgur.com/vdVCmvo.gif\" width=50% />\n",
    "\n",
    "**Pong** é o ambiente de Aprendizado por Reforço criado pelo Turing que simula o jogo de *Pong*, no qual existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes é não somente evitar que a bola passe por ela, como também fazer com que esta passe pela linha que a outra raquete protege."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 2 informações.\n",
    "\n",
    "| Estado    | Informação                            |\n",
    "| :-------- | :------------------------------------ |\n",
    "| 0         | Distância _x_ entre a bola e o agente |\n",
    "| 1         | Distância _y_ entre a bola e o agente |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por três ações: mover o jogador para cima, baixo, ou deixá-lo parado.\n",
    "\n",
    "| Ação | Significado      |\n",
    "| :--- | :--------------- |\n",
    "| 0    | Ficar parado     |\n",
    "| 1    | Mover para baixo |\n",
    "| 2    | Mover para cima  |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência          | Recompensa |\n",
    "| :------------------ | ---------: |\n",
    "| Ponto do Agente     | $+500$     |\n",
    "| Ponto do Oponente   | $-500$     |\n",
    "| Vitória do Agente   | $+2000$    |\n",
    "| Vitória do Oponente | $-2000$    |\n",
    "\n",
    "O primeiro jogador a fazer quatro pontos ganha o jogo. Além disso, as recompensas são cumulativas. Isso significa que se o oponente fizer um ponto _e_ ganhar o jogo, a recompensa é de $-2500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes criados pelo Turing, basta rodar o seguinte comando no notebook (se preferir, também pode rodar no terminal; é só tirar o ponto de exclamação do começo da linha):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: turing-envs in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: gym in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from turing-envs) (0.18.0)\n",
      "Requirement already satisfied: pygame in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from turing-envs) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gym->turing-envs) (1.5.4)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gym->turing-envs) (7.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gym->turing-envs) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gym->turing-envs) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gym->turing-envs) (1.20.3)\n",
      "Requirement already satisfied: future in c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->turing-envs) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U turing-envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o ambiente do Turing instalado, podemos testar um modelo! Para isso, vamos precisar de 2 bibliotecas: **gym** (para inicialização dos ambientes) e **stable_baselines3** (para inicialização e avaliação dos modelos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "c:\\users\\felammachado\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Recompensa Média: 3900.00 +/- 200.0\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Definindo ambiente\n",
    "env = gym.make(\"turing_envs:pong-easy-v0\")\n",
    "\n",
    "# Definindo modelo\n",
    "model = PPO('MlpPolicy', env, seed=1, verbose=1)\n",
    "\n",
    "# Avaliando o agente\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa Média: {mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Lunar Lander</h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif\" width=50% />\n",
    "\n",
    "**Lunar Lander** é um ambiente do Gym que simula o pouso de um módulo lunar na Lua. O agente deve controlar os três motores do módulo para guiá-lo até a pista de pouso, sem gastar muita energia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 8 informações.\n",
    "\n",
    "| Estado    | Informação                                     |\n",
    "| :-------- | :--------------------------------------------- |\n",
    "| 0         | Posição no eixo _x_ do módulo                  |\n",
    "| 1         | Posição no eixo _y_ do módulo                  |\n",
    "| 2         | Velocidade no eixo _x_ do módulo               |\n",
    "| 3         | Velocidade no eixo _y_ do módulo               |\n",
    "| 4         | Ângulo do módulo                               |\n",
    "| 5         | Velocidade angular do módulo                   |\n",
    "| 6         | Se a perna esquerda está em contato com o chão |\n",
    "| 7         | Se a perna direita está em contato com o chão  |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por quatro ações: não fazer nada, acionar o motor esquerdo, acionar o motor principal ou acionar o motor direito.\n",
    "\n",
    "| Ação | Significado             |\n",
    "| :--- | :---------------------- |\n",
    "| 0    | Não fazer nada          |\n",
    "| 1    | Acionar motor esquerdo  |\n",
    "| 2    | Acionar motor principal |\n",
    "| 3    | Acionar motor direito   |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência              | Recompensa       |\n",
    "| :---------------------- | ---------------: |\n",
    "| Se aproximar da pista   | Até $+140$       |\n",
    "| Pousar                  | $+100$           |\n",
    "| Colidir                 | $-100$           |\n",
    "| Tocar uma perna no chão | $+10$            |\n",
    "| Acionar motor principal | $-0.3$ por frame |\n",
    "\n",
    "#### Lunar Lander Continuous\n",
    "\n",
    "Também existe uma versão contínua do ambiente do Lunar Lander, no qual podemos controlar a força que cada um dos motores do módulo exercerá. Nesse caso, teremos duas ações:\n",
    "\n",
    "| Ação | Intervalo  | Significado                          |\n",
    "| :--- | :--------: | :----------------------------------- |\n",
    "| 0    | $-1$ a $1$ | Força do motor principal             |\n",
    "| 1    | $-1$ a $1$ | Força dos motores esquerdo e direito |\n",
    "\n",
    "Na versão contínua, os algoritmos que poderemos usar serão diferentes, e o treinamento provavlmente será mais difícil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes do Gym que usam a engine Box2D, é necessário rodar os seguintes comandos numa célula do notebook:\n",
    "\n",
    "\n",
    "**Windows**\n",
    "```bat\n",
    "!conda install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]\n",
    "```\n",
    "\n",
    "**Linux**\n",
    "```bash\n",
    "!apt install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]\n",
    "```\n",
    "\n",
    "Em seguida, para criar o ambiente, roda-se a linha de código a seguir:\n",
    "\n",
    "```python\n",
    "env = gym.make('LunarLander-v2')\n",
    "```\n",
    "\n",
    "ou\n",
    "\n",
    "```python\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Testando Modelos\n",
    "\n",
    "Caro piloto, agora que você conhece esses dois ambientes, é hora de brincar com eles. Você deverá testar diferentes algoritmos, e ver sua recompensa média. Para ver quais as limitações dos modelos, veja esse link (linkar baselines). Abaixo, criamos uma função que será útil para comparar os modelos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(model, env, n_episodes, info_dict):\n",
    "    model_name = str(model.__class__).split('.')[-1][:-2]\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes, deterministic=True)\n",
    "    \n",
    "    info_dict[model_name] = {}\n",
    "    info_dict[model_name][\"mean_reward\"] = mean_reward\n",
    "    info_dict[model_name][\"std_reward\"] = std_reward\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "discrete_dict = {}\n",
    "continuous_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso Discreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "model = ... # Defina o modelo \n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente\n",
    "discrete_dict = getValues(model, env, n_episodes, discrete_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço para o Piloto criar gráficos ou textos para mostrar os diferentes resultados entre modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso Contínuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "model = ... # Defina o modelo \n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente\n",
    "continuous_dict = getValues(model, env, n_episodes, discrete_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço para o Piloto criar gráficos ou textos para mostrar os diferentes resultados entre modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do Algoritmo\n",
    "\n",
    " - Por que escolheu o algoritmo?\n",
    "   - Motivo pode ser baseado na teoria ou na prática.\n",
    " - Testou mais de um?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2 - Mudança de Hiperparâmetros\n",
    "\n",
    " - Interpretar as mudanças de _pelo menos_ 2 hiperparâmetros:\n",
    "   - Sugestões: `gamma`, `learning_rate`.\n",
    "   - Tente pensar e elaborar alguma teoria explicando os resultados encontrados.\n",
    " - [~~~Testar Policy Networks diferentes?~~~](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValuesAndParams(model, env, n_episodes, info_dict):\n",
    "    model_name = str(model.__class__).split('.')[-1][:-2]\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes, deterministic=True)\n",
    "    \n",
    "    if model_name not in info_dict.keys():\n",
    "        info_dict[model_name] = {}\n",
    "        \n",
    "    if \"mean_reward\" not in info_dict[model_name].keys():\n",
    "        info_dict[model_name][\"mean_reward\"] = []\n",
    "        info_dict[model_name][\"std_reward\"] = []\n",
    "        \n",
    "    info_dict[model_name][\"mean_reward\"].append(mean_reward)\n",
    "    info_dict[model_name][\"std_reward\"].append(std_reward)\n",
    "    \n",
    "    params_dict = model.get_parameters()\n",
    "    if 'policy.optimizer' not in params_dict.keys():\n",
    "        if \"actor.optimizer\" not in info_dict[model_name].keys():\n",
    "            info_dict[model_name][\"actor.optimizer\"] = []\n",
    "            info_dict[model_name][\"critic.optimizer\"] = []\n",
    "\n",
    "        info_dict[model_name][\"actor.optimizer\"] += params_dict[\"actor.optimizer\"]['param_groups']\n",
    "        info_dict[model_name][\"critic.optimizer\"] += params_dict[\"critic.optimizer\"]['param_groups']\n",
    "    \n",
    "    else:\n",
    "        if \"policy.optimizer\" not in info_dict[model_name].keys():\n",
    "            info_dict[model_name][\"policy.optimizer\"] = []\n",
    "    \n",
    "        info_dict[model_name][\"policy.optimizer\"] += params_dict[\"policy.optimizer\"]['param_groups']\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "discrete_parameters_dict = {}\n",
    "continuous_parameters_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso Discreto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "model = ... # Defina o modelo \n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente\n",
    "discrete_parameters_dict = getValuesAndParams(model, env, n_episodes, discrete_parameters_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso Contínuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "\n",
    "model = ... # Defina o modelo \n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente\n",
    "continuous_parameters_dict = getValuesAndParams(model, env, n_episodes, continuous_parameters_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
