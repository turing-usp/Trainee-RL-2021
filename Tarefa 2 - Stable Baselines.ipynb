{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/3U3hI1u.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> descrever aqui em que consiste a tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do Ambiente\n",
    "\n",
    "Antes de analisar o possíveis algoritmos, o primeiro passo é escolher qual ambiente você quer resolver! Para esta tarefa, separamos quatro possíveis ambientes diferentes, em ordem de dificuldade, que você poderá escolher: **Pong**, **Lunar Lander**, **Lunar Lander Continuous** e **Car Racing**.\n",
    "\n",
    "A seguir, estão as descrições de cada um deles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Pong</h2>\n",
    "<img src=\"https://imgur.com/vdVCmvo.gif\" width=50% />\n",
    "\n",
    "**Pong** é o ambiente de Aprendizado por Reforço criado pelo Turing que simula o jogo de *Pong*, no qual existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes é não somente evitar que a bola passe por ela, como também fazer com que esta passe pela linha que a outra raquete protege."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 2 informações.\n",
    "\n",
    "| Estado    | Informação                            |\n",
    "| :-------- | :------------------------------------ |\n",
    "| 0         | Distância _x_ entre a bola e o agente |\n",
    "| 1         | Distância _y_ entre a bola e o agente |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por três ações: mover o jogador para cima, baixo, ou deixá-lo parado.\n",
    "\n",
    "| Ação | Significado      |\n",
    "| :--- | :--------------- |\n",
    "| 0    | Ficar parado     |\n",
    "| 1    | Mover para baixo |\n",
    "| 2    | Mover para cima  |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência          | Recompensa |\n",
    "| :------------------ | ---------: |\n",
    "| Ponto do Agente     | $+500$     |\n",
    "| Ponto do Oponente   | $-500$     |\n",
    "| Vitória do Agente   | $+2000$    |\n",
    "| Vitória do Oponente | $-2000$    |\n",
    "\n",
    "O primeiro jogador a fazer quatro pontos ganha o jogo. Além disso, as recompensas são cumulativas. Isso significa que se o oponente fizer um ponto _e_ ganhar o jogo, a recompensa é de $-2500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes criados pelo Turing, basta rodar o seguinte comando no terminal:\n",
    "\n",
    "```cmd\n",
    "!pip install -U turing-envs\n",
    "```\n",
    "\n",
    "Em seguida, para criar o ambiente, roda-se a linha de código a seguir:\n",
    "\n",
    "```python\n",
    "env = gym.make(\"turing_envs:pong-easy-v0\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Lunar Lander</h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif\" width=50% />\n",
    "\n",
    "**Lunar Lander** é um ambiente do Gym que simula o pouso de um módulo lunar na Lua. O agente deve controlar os três motores do módulo para guiá-lo até a pista de pouso, sem gastar muita energia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 8 informações.\n",
    "\n",
    "| Estado    | Informação                                     |\n",
    "| :-------- | :--------------------------------------------- |\n",
    "| 0         | Posição no eixo _x_ do módulo                  |\n",
    "| 1         | Posição no eixo _y_ do módulo                  |\n",
    "| 2         | Velocidade no eixo _x_ do módulo               |\n",
    "| 3         | Velocidade no eixo _y_ do módulo               |\n",
    "| 4         | Ângulo do módulo                               |\n",
    "| 5         | Velocidade angular do módulo                   |\n",
    "| 6         | Se a perna esquerda está em contato com o chão |\n",
    "| 7         | Se a perna direita está em contato com o chão  |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por quatro ações: não fazer nada, acionar o motor esquerdo, acionar o motor principal ou acionar o motor direito.\n",
    "\n",
    "| Ação | Significado             |\n",
    "| :--- | :---------------------- |\n",
    "| 0    | Não fazer nada          |\n",
    "| 1    | Acionar motor esquerdo  |\n",
    "| 2    | Acionar motor principal |\n",
    "| 3    | Acionar motor direito   |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência              | Recompensa       |\n",
    "| :---------------------- | ---------------: |\n",
    "| Se aproximar da pista   | Até $+140$       |\n",
    "| Pousar                  | $+100$           |\n",
    "| Colidir                 | $-100$           |\n",
    "| Tocar uma perna no chão | $+10$            |\n",
    "| Acionar motor principal | $-0.3$ por frame |\n",
    "\n",
    "#### Lunar Lander Continuous\n",
    "\n",
    "Também existe uma versão contínua do ambiente do Lunar Lander, no qual podemos controlar a força que cada um dos motores do módulo exercerá. Nesse caso, teremos duas ações:\n",
    "\n",
    "| Ação | Intervalo  | Significado                          |\n",
    "| :--- | :--------: | :----------------------------------- |\n",
    "| 0    | $-1$ a $1$ | Força do motor principal             |\n",
    "| 1    | $-1$ a $1$ | Força dos motores esquerdo e direito |\n",
    "\n",
    "Na versão contínua, os algoritmos que poderemos usar serão diferentes, e o treinamento provavlmente será mais difícil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes do Gym que usam a engine Box2D, é necessário rodar os seguintes comandos no terminal:\n",
    "\n",
    "\n",
    "**Windows**\n",
    "```cmd\n",
    "!conda install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]\n",
    "```\n",
    "\n",
    "**Linux**\n",
    "```bash\n",
    "!apt install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]\n",
    "```\n",
    "\n",
    "Em seguida, para criar o ambiente, roda-se a linha de código a seguir:\n",
    "\n",
    "```python\n",
    "env = gym.make('LunarLander-v2')\n",
    "```\n",
    "\n",
    "ou\n",
    "\n",
    "```python\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Car Racing</h2>\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/10dd9fe65857382138418a936d23dace67c2f6d158b5dc0194c5219ef57d7c65/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f336f673049454b753834526f7339697a79552f67697068792e676966\" width=50% />\n",
    "\n",
    "**Car Racing** é um ambiente do Gym que simula um carro de corrida em uma pista. O agente deve pilotar o carro o mais rápido possível pela pista, até completar uma volta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é uma imagem RGB de 96x96 pixels da vista superior do carro, como exemplificado a seguir:\n",
    "\n",
    "<img src=\"https://imgur.com/7UFUngq.png\" width=30% />\n",
    "\n",
    "Já o **Espaço de Ação** é composto por três ações contínuas: o controle do volante, do acelerador e do freio.\n",
    "\n",
    "| Ação | Intervalo  | Significado |\n",
    "| :--- | :--------: | :---------- |\n",
    "| 0    | $-1$ a $1$ | Volante     |\n",
    "| 1    | $0$ a $1$  | Acelerador  |\n",
    "| 2    | $0$ a $1$  | Freio       |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência               | Recompensa                                           |\n",
    "| :----------------------- | ---------------------------------------------------: |\n",
    "| Passar um bloco da pista | $\\dfrac{+1000}{N}$ por bloco ($N$ = total de blocos) |\n",
    "| A cada instante de tempo | $-0.1$                                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes do Gym que usam a engine Box2D, é necessário rodar os seguintes comandos no terminal:\n",
    "\n",
    "\n",
    "**Windows**\n",
    "```cmd\n",
    "!conda install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]\n",
    "```\n",
    "\n",
    "**Linux**\n",
    "```bash\n",
    "!apt install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]\n",
    "```\n",
    "\n",
    "Em seguida, para criar o ambiente, roda-se a linha de código a seguir:\n",
    "\n",
    "```python\n",
    "env = gym.make('CarRacing-v0')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do Algoritmo\n",
    "\n",
    " - Por que escolheu o algoritmo?\n",
    "   - Motivo pode ser baseado na teoria ou na prática.\n",
    " - Testou mais de um?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização de Hiperparâmetros\n",
    "\n",
    " - Interpretar as mudanças de _pelo menos_ 2 hiperparâmetros:\n",
    "   - Sugestões: `gamma`, `learning_rate`.\n",
    "   - Tente pensar e elaborar alguma teoria explicando os resultados encontrados.\n",
    " - [~~~Testar Policy Networks diferentes?~~~](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
