{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/3U3hI1u.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boas vindas a sua segunda e última tarefa de Aprendizado por Reforço!\n",
    "\n",
    "Neste exercício, você deverá implementar e comparar diferentes algoritmos de **Aprendizado por Reforço Profundo** utilizando a biblioteca _[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)_.\n",
    "\n",
    "A _Stable Baselines_ é uma biblioteca de Aprendizado por Reforço que implementa diversos algoritmos de agentes, além de várias funcionalidades úteis para seu treinamento. Suas implementações são bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de reforço de alta qualidade.\n",
    "\n",
    "Antes de começar a tarefa, é importante acessar e se familiarizar com o tutorial da biblioteca disponível neste repositório! Depois de rodar o guia, você já estará capaz de completar este trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do Ambiente\n",
    "\n",
    "Antes de analisar o possíveis algoritmos, o primeiro passo é escolher qual ambiente você quer resolver! Para esta tarefa, separamos quatro possíveis ambientes diferentes, em ordem de dificuldade, que você poderá escolher: **Pong**, **Lunar Lander** e **Lunar Lander Continuous**. Lembrando que, quanto mais difícil um ambiente, mais demorado será o treinamento.\n",
    "\n",
    "A seguir, estão as descrições de cada um deles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Pong</h2>\n",
    "<img src=\"https://imgur.com/vdVCmvo.gif\" width=50% />\n",
    "\n",
    "**Pong** é o ambiente de Aprendizado por Reforço criado pelo Turing que simula o jogo de *Pong*, no qual existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes é não somente evitar que a bola passe por ela, como também fazer com que esta passe pela linha que a outra raquete protege."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 2 informações.\n",
    "\n",
    "| Estado    | Informação                            |\n",
    "| :-------- | :------------------------------------ |\n",
    "| 0         | Distância _x_ entre a bola e o agente |\n",
    "| 1         | Distância _y_ entre a bola e o agente |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por três ações: mover o jogador para cima, baixo, ou deixá-lo parado.\n",
    "\n",
    "| Ação | Significado      |\n",
    "| :--- | :--------------- |\n",
    "| 0    | Ficar parado     |\n",
    "| 1    | Mover para baixo |\n",
    "| 2    | Mover para cima  |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência          | Recompensa |\n",
    "| :------------------ | ---------: |\n",
    "| Ponto do Agente     | $+500$     |\n",
    "| Ponto do Oponente   | $-500$     |\n",
    "| Vitória do Agente   | $+2000$    |\n",
    "| Vitória do Oponente | $-2000$    |\n",
    "\n",
    "O primeiro jogador a fazer quatro pontos ganha o jogo. Além disso, as recompensas são cumulativas. Isso significa que se o oponente fizer um ponto _e_ ganhar o jogo, a recompensa é de $-2500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes criados pelo Turing, basta rodar o seguinte comando no notebook (se preferir, também pode rodar no terminal; é só tirar o ponto de exclamação do começo da linha):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U turing-envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o ambiente do Turing instalado, podemos testar um modelo! Para isso, vamos precisar de 2 bibliotecas: **gym** (para inicialização dos ambientes) e **stable_baselines3** (para inicialização e avaliação dos modelos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Definindo ambiente\n",
    "env = gym.make(\"turing_envs:pong-normal-v0\")\n",
    "\n",
    "# Definindo modelo\n",
    "model = PPO('MlpPolicy', env, seed=1, verbose=1)\n",
    "\n",
    "# Avaliando o agente\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa Média: {mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Lunar Lander</h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif\" width=50% />\n",
    "\n",
    "**Lunar Lander** é um ambiente do Gym que simula o pouso de um módulo lunar na Lua. O agente deve controlar os três motores do módulo para guiá-lo até a pista de pouso, sem gastar muita energia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 8 informações.\n",
    "\n",
    "| Estado    | Informação                                     |\n",
    "| :-------- | :--------------------------------------------- |\n",
    "| 0         | Posição no eixo _x_ do módulo                  |\n",
    "| 1         | Posição no eixo _y_ do módulo                  |\n",
    "| 2         | Velocidade no eixo _x_ do módulo               |\n",
    "| 3         | Velocidade no eixo _y_ do módulo               |\n",
    "| 4         | Ângulo do módulo                               |\n",
    "| 5         | Velocidade angular do módulo                   |\n",
    "| 6         | Se a perna esquerda está em contato com o chão |\n",
    "| 7         | Se a perna direita está em contato com o chão  |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por quatro ações: não fazer nada, acionar o motor esquerdo, acionar o motor principal ou acionar o motor direito.\n",
    "\n",
    "| Ação | Significado             |\n",
    "| :--- | :---------------------- |\n",
    "| 0    | Não fazer nada          |\n",
    "| 1    | Acionar motor esquerdo  |\n",
    "| 2    | Acionar motor principal |\n",
    "| 3    | Acionar motor direito   |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência              | Recompensa       |\n",
    "| :---------------------- | ---------------: |\n",
    "| Se aproximar da pista   | Até $+140$       |\n",
    "| Pousar                  | $+100$           |\n",
    "| Colidir                 | $-100$           |\n",
    "| Tocar uma perna no chão | $+10$            |\n",
    "| Acionar motor principal | $-0.3$ por frame |\n",
    "\n",
    "#### Lunar Lander Continuous\n",
    "\n",
    "Também existe uma versão contínua do ambiente do Lunar Lander, no qual podemos controlar a força que cada um dos motores do módulo exercerá. Nesse caso, teremos duas ações:\n",
    "\n",
    "| Ação | Intervalo  | Significado                          |\n",
    "| :--- | :--------: | :----------------------------------- |\n",
    "| 0    | $-1$ a $+1$ | Força do motor principal             |\n",
    "| 1    | $-1$ a $+1$ | Força dos motores esquerdo e direito |\n",
    "\n",
    "Na versão contínua, os algoritmos que poderemos usar serão diferentes, e o treinamento provavlmente será mais difícil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes do Gym que usam a engine Box2D, é necessário rodar os seguintes comandos numa célula do notebook (se preferir, também pode rodar no terminal; é só tirar o ponto de exclamação do começo da linha):\n",
    "\n",
    "\n",
    "**Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install swig\n",
    "!pip install Box2D\n",
    "!pip install pyglet==1.2.4\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, para criar o ambiente, roda-se a linha de código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Testando Modelos\n",
    "\n",
    "Caro piloto, agora que você conhece esses dois ambientes, é hora de brincar com eles. Você deverá testar diferentes algoritmos (a seu critério), e ver sua recompensa média. Para ver quais as limitações dos modelos, veja esse [link](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html). Abaixo, criamos uma função que será útil para comparar os modelos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(model, env, n_episodes, info_dict):\n",
    "    model_name = str(model.__class__).split('.')[-1][:-2]\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes, deterministic=True)\n",
    "    \n",
    "    info_dict[model_name] = {}\n",
    "    info_dict[model_name][\"mean_reward\"] = mean_reward\n",
    "    info_dict[model_name][\"std_reward\"] = std_reward\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "algorithms_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e Avaliando seu próprio modelo\n",
    "\n",
    "Primeiramente, agora você deve decidir em qual ambiente você deseja treinar seu agente. Para isto, basta tirar o comentário da linha referente ao ambiente escolhido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"turing_envs:pong-normal-v0\"\n",
    "# env_name = \"LunarLander-v2\"\n",
    "# env_name = \"LunarLanderContinuous-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, você está livre para testar diferentes algoritmos para seu ambiente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import ... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo o ambiente\n",
    "env = gym.make(env_name)\n",
    "\n",
    "model = ... # Defina o modelo\n",
    "model.learn(total_timesteps=...) # Treine o modelo\n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente e guardando o desempenho no dicionário\n",
    "algorithms_dict = getValues(model, env, n_episodes, algorithms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço livre para testagem de diferentes algoritmos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, detalhe um pouco mais quais foram os algoritmos testados bem como a performance obtida por cada um.\n",
    "\n",
    "Este detalhamento pode ser feito por meio de um ou mais gráficos mostrando o desempenho dos modelos, ou simplesmente por texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço para o Piloto criar gráficos ou textos para mostrar os diferentes resultados entre modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do Algoritmo\n",
    "\n",
    "Após testar e analisar diversos algoritmos diferentes, qual foi o escolhido?\n",
    "\n",
    "_Pergunta Extra:_ você usou algum critério para escolher quais algoritmos seriam testados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Qual foi o algoritmo escolhido?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2 - Mudança de Hiperparâmetros\n",
    "\n",
    "No segundo exercício, você deve testar diferentes hiperparâmetros para seu modelo escolhido!\n",
    "\n",
    "Todos os modelos da Stable Baselines têm diversos parâmetros detalhados em sua documentação ([exemplo: PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters)) que podem ser modificados para alterar o modelo.\n",
    "\n",
    "Para modificar os parâmetros de um algoritmo, tal como o ```PPO(...)```, basta especificá-los durante a construção do modelo, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, seed=1, verbose=1,\n",
    "           learning_rate=0.0007, gamma=0.9) # Especificando diferentes valores para a learning_rate e para o gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quiser, você também pode alterar a arquitetura das redes neurais do modelo, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "\n",
    "# Parâmetros das redes neurais\n",
    "policy_kwargs = dict(activation_fn=torch.nn.ReLU,                # Troca a função de ativação para ReLU\n",
    "                     net_arch=[dict(pi=[32, 32], vf=[32, 32])])  # Define a arquitetura das redes do Actor-Critic\n",
    "\n",
    "# Cria o nosso modelo com os novos parâmetros\n",
    "model = PPO('MlpPolicy', env, seed=1, verbose=1, \n",
    "            policy_kwargs=policy_kwargs) # Especificando outra arquitetura de rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso queira ler um pouco mais sobre essas diferentes arquiteturas, recomendamos [a seguinte página da documentação da biblioteca.](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)\n",
    "\n",
    "A seguir, preparamos uma outra função útil para guardar os parâmetros testados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValuesAndParams(model, env, n_episodes, info_dict):\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "    env_name = env.unwrapped.__class__.__name__\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes, deterministic=True)\n",
    "    \n",
    "    args = inspect.getfullargspec(model.__class__).args\n",
    "    func_params_dict = {}\n",
    "    for arg in args:\n",
    "        p = model.__dict__.get(arg)\n",
    "        if type(p) in [None, int, float, str, dict]:\n",
    "            func_params_dict[arg] = p\n",
    "    \n",
    "    \n",
    "    if model_name not in info_dict:\n",
    "        info_dict[model_name] = {}\n",
    "        \n",
    "    if \"env\" not in info_dict[model_name]:\n",
    "        info_dict[model_name][\"env\"] = []\n",
    "        \n",
    "    info_dict[model_name][\"env\"].append(env_name)\n",
    "        \n",
    "    if \"mean_reward\" not in info_dict[model_name]:\n",
    "        info_dict[model_name][\"mean_reward\"] = []\n",
    "        info_dict[model_name][\"std_reward\"] = []\n",
    "        \n",
    "    if \"class_args\" not in info_dict[model_name]:\n",
    "        info_dict[model_name][\"class_args\"] = []\n",
    "        \n",
    "    info_dict[model_name][\"mean_reward\"].append(mean_reward)\n",
    "    info_dict[model_name][\"std_reward\"].append(std_reward)\n",
    "    info_dict[model_name][\"class_args\"].append(func_params_dict)\n",
    "    \n",
    "    params_dict = model.get_parameters()\n",
    "    if 'policy.optimizer' not in params_dict:\n",
    "        if \"actor.optimizer\" not in info_dict[model_name]:\n",
    "            info_dict[model_name][\"actor.optimizer\"] = []\n",
    "            info_dict[model_name][\"critic.optimizer\"] = []\n",
    "\n",
    "        info_dict[model_name][\"actor.optimizer\"] += params_dict[\"actor.optimizer\"]['param_groups']\n",
    "        info_dict[model_name][\"critic.optimizer\"] += params_dict[\"critic.optimizer\"]['param_groups']\n",
    "    \n",
    "    else:\n",
    "        if \"policy.optimizer\" not in info_dict[model_name]:\n",
    "            info_dict[model_name][\"policy.optimizer\"] = []\n",
    "    \n",
    "        info_dict[model_name][\"policy.optimizer\"] += params_dict[\"policy.optimizer\"]['param_groups']\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "parameters_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, você está livre para testar diferentes hiperparâmetros para o algoritmo escolhido! Você deve testar valores diversos para pelo menos **dois** parâmetros. Se quiser algumas sugestões, recomendamos realizar testes no `gamma`, na `learning_rate` ou na arquitetura da rede neural do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import ... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo o ambiente\n",
    "env = ...\n",
    "\n",
    "model = ... # Defina o modelo\n",
    "model.learn(total_timesteps=...) # Treine o modelo\n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente\n",
    "parameters_dict = getValuesAndParams(model, env, n_episodes, parameters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o ambiente\n",
    "env = ...\n",
    "\n",
    "model = ... # Defina o modelo\n",
    "model.learn(total_timesteps=...) # Treine o modelo\n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente\n",
    "parameters_dict = getValuesAndParams(model, env, n_episodes, parameters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço livre para a comparação dos hiperparâmetros\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
