{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/3U3hI1u.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> descrever aqui em que consiste a tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "<h2 align=\"center\">Pong</h2>\n",
    "2\n",
    "<h2>\n",
    "    <center>Pong</center>\n",
    "</h2>\n",
    "3\n",
    "<h2 style=\"text-align:center;\" >Pong</h2>\n",
    "<img src=\"https://imgur.com/vdVCmvo.gif\" width=50% />\n",
    "\n",
    "**Pong** é o ambiente de Aprendizado por Reforço criado pelo Turing que simula o jogo de *Pong*, no qual existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes é não somente evitar que a bola passe por ela, como também fazer com que esta passe pela linha que a outra raquete protege."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 2 informações.\n",
    "\n",
    "| Estado    | Informação                            |\n",
    "| :-------- | :------------------------------------ |\n",
    "| 0         | Distância _x_ entre a bola e o agente |\n",
    "| 1         | Distância _y_ entre a bola e o agente |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por três ações: mover o jogador para cima, baixo, ou deixá-lo parado.\n",
    "\n",
    "| Ação | Significado      |\n",
    "| :--- | :--------------- |\n",
    "| 0    | Ficar parado     |\n",
    "| 1    | Mover para baixo |\n",
    "| 2    | Mover para cima  |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência          | Recompensa |\n",
    "| :------------------ | ---------: |\n",
    "| Ponto do Agente     | $+500$     |\n",
    "| Ponto do Oponente   | $-500$     |\n",
    "| Vitória do Agente   | $+2000$    |\n",
    "| Vitória do Oponente | $-2000$    |\n",
    "\n",
    "O primeiro jogador a fazer quatro pontos ganha o jogo. Além disso, as recompensas são cumulativas. Isso significa que se o oponente fizer um ponto _e_ ganhar o jogo, a recompensa é de $-2500$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala os ambientes do Turing\n",
    "\n",
    "# !pip install -U turing-envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"turing_envs:pong-easy-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2><center>Lunar Lander</center></h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif\" width=50% />\n",
    "\n",
    "> Detalhes do ambiente aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do Algoritmo\n",
    "\n",
    " - Por que escolheu o algoritmo?\n",
    "   - Motivo pode ser baseado na teoria ou na prática.\n",
    " - Testou mais de um?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização de Hiperparâmetros\n",
    "\n",
    " - Interpretar as mudanças de _pelo menos_ 2 hiperparâmetros:\n",
    "   - Sugestões: `gamma`, `learning_rate`.\n",
    "   - Tente pensar e elaborar alguma teoria explicando os resultados encontrados.\n",
    " - [~~~Testar Policy Networks diferentes?~~~](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
