{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/3U3hI1u.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boas vindas a sua segunda e última tarefa de Aprendizado por Reforço!\n",
    "\n",
    "Neste exercício, você deverá implementar e comparar diferentes algoritmos de **Aprendizado por Reforço Profundo** utilizando a biblioteca _[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)_.\n",
    "\n",
    "A _Stable Baselines_ é uma biblioteca de Aprendizado por Reforço que implementa diversos algoritmos de agentes, além de várias funcionalidades úteis para seu treinamento. Suas implementações são bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de reforço de alta qualidade.\n",
    "\n",
    "Antes de começar a tarefa, é importante acessar e se familiarizar com o tutorial da biblioteca disponível neste repositório! Depois de rodar o guia, você já estará capaz de completar este trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do Ambiente\n",
    "\n",
    "Antes de analisar o possíveis algoritmos, o primeiro passo é escolher qual ambiente você quer resolver! Para esta tarefa, separamos dois possíveis ambientes diferentes, em ordem de dificuldade, que você poderá escolher: **CartPole** e **Pendulum**. Lembrando que, quanto mais difícil um ambiente, mais demorado será o treinamento.\n",
    "\n",
    "A seguir, estão as descrições de cada um deles:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">CartPole</h2>\n",
    "<img src=\"https://bytepawn.com/images/cartpole.gif\" width=50% />\n",
    "\n",
    "**CartPole** é o ambiente de Aprendizado por Reforço mais comum do Gym, no qual deve-se balancear um pêndulo invertido conectado a um carrinho, somente controlando os movimentos do carrinho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do CartPole é definido por 4 informações:\n",
    "\n",
    "<br>\n",
    "\n",
    "|     | Informação                         | Min     | Max    |\n",
    "| :-- | :--------------------------------- | :-----: | :----: |\n",
    "| 0   | Posição do Carrinho                | -4.8    | 4.8    |\n",
    "| 1   | Velocidade do Carrinho             | -Inf    | Inf    |\n",
    "| 2   | Ângulo da Barra                    | -24 deg | 24 deg |\n",
    "| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |\n",
    "\n",
    "<br>\n",
    "\n",
    "A posição do carrinho vai de -4.8 a 4.8, mas ele perde o episódio caso saia dos limites de -2.4 e 2.4. Da mesma forma, o ângulo da barra vai de -24° a 24°, porém o episódio acaba caso a barra saia dos limites de -12° e 12°.\n",
    "\n",
    "Já o **Espaço de Ação** é composto por duas ações únicas: mover o carrinho para a **esquerda** ou para a **direita**.\n",
    "\n",
    "Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos movê-lo para a direita, enviamos um `env.step(1)`\n",
    "\n",
    "| Ação | Significado           |\n",
    "| :--- | :-------------------- |\n",
    "| 0    | Mover para a esquerda |\n",
    "| 1    | Mover para a direito  |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, que é igual a +1 para cada instante que passa sem o agente perder. Assim, o CartPole é incentivado a sobreviver por mais tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar o modelo, vamos precisar de 2 bibliotecas: **gym** (para inicialização dos ambientes) e **stable_baselines3** (para inicialização e avaliação dos modelos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Definindo ambiente\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Definindo modelo\n",
    "model = PPO(\"MlpPolicy\", env, seed=1, verbose=1)\n",
    "\n",
    "# Avaliando o agente\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "\n",
    "print(f\"Recompensa Média: {mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Lunar Lander</h2>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fakemonk1/Reinforcement-Learning-Lunar_Lander/master/images/3.gif\" width=50% />\n",
    "\n",
    "**Lunar Lander** é um ambiente do Gym que simula o pouso de um módulo lunar na Lua. O agente deve controlar os três motores do módulo para guiá-lo até a pista de pouso, sem gastar muita energia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características do Ambiente\n",
    "\n",
    "O **Espaço de Observação** do ambiente é definido por 8 informações.\n",
    "\n",
    "| Estado    | Informação                                     |\n",
    "| :-------- | :--------------------------------------------- |\n",
    "| 0         | Posição no eixo _x_ do módulo                  |\n",
    "| 1         | Posição no eixo _y_ do módulo                  |\n",
    "| 2         | Velocidade no eixo _x_ do módulo               |\n",
    "| 3         | Velocidade no eixo _y_ do módulo               |\n",
    "| 4         | Ângulo do módulo                               |\n",
    "| 5         | Velocidade angular do módulo                   |\n",
    "| 6         | Se a perna esquerda está em contato com o chão |\n",
    "| 7         | Se a perna direita está em contato com o chão  |\n",
    "\n",
    "Já o **Espaço de Ação** é composto por quatro ações: não fazer nada, acionar o motor esquerdo, acionar o motor principal ou acionar o motor direito.\n",
    "\n",
    "| Ação | Significado             |\n",
    "| :--- | :---------------------- |\n",
    "| 0    | Não fazer nada          |\n",
    "| 1    | Acionar motor esquerdo  |\n",
    "| 2    | Acionar motor principal |\n",
    "| 3    | Acionar motor direito   |\n",
    "\n",
    "Por fim, cada vez que tomamos uma ação, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
    "\n",
    "| Ocorrência              | Recompensa       |\n",
    "| :---------------------- | ---------------: |\n",
    "| Se aproximar da pista   | Até $+140$       |\n",
    "| Pousar                  | $+100$           |\n",
    "| Colidir                 | $-100$           |\n",
    "| Tocar uma perna no chão | $+10$            |\n",
    "| Acionar motor principal | $-0.3$ por frame |\n",
    "\n",
    "#### Lunar Lander Continuous\n",
    "\n",
    "Também existe uma versão contínua do ambiente do Lunar Lander, no qual podemos controlar a força que cada um dos motores do módulo exercerá. Nesse caso, teremos duas ações:\n",
    "\n",
    "| Ação | Intervalo  | Significado                          |\n",
    "| :--- | :--------: | :----------------------------------- |\n",
    "| 0    | $-1$ a $+1$ | Força do motor principal             |\n",
    "| 1    | $-1$ a $+1$ | Força dos motores esquerdo e direito |\n",
    "\n",
    "Na versão contínua, os algoritmos que poderemos usar serão diferentes, e o treinamento provavlmente será mais difícil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "\n",
    "Para instalar os ambientes do Gym que usam a engine Box2D, é necessário rodar os seguintes comandos numa célula do notebook (se preferir, também pode rodar no terminal; é só tirar o ponto de exclamação do começo da linha):\n",
    "\n",
    "\n",
    "**Windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install swig\n",
    "!pip install box2d box2d-kengz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install swig\n",
    "!pip install -u 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, para criar o ambiente, roda-se a linha de código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1 - Testando Modelos\n",
    "\n",
    "Caro piloto, agora que você conhece esses dois ambientes, é hora de brincar com eles. Você deverá testar diferentes algoritmos (a seu critério), e ver sua recompensa média. Para ver quais as limitações dos modelos, veja esse [link](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html). Abaixo, criamos uma função que será útil para comparar os modelos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(model, env, n_episodes, info_dict):\n",
    "    model_name = str(model.__class__).split(\".\")[-1][:-2]\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_episodes, deterministic=True)\n",
    "    \n",
    "    info_dict[model_name] = {}\n",
    "    info_dict[model_name][\"mean_reward\"] = mean_reward\n",
    "    info_dict[model_name][\"std_reward\"] = std_reward\n",
    "    \n",
    "    return info_dict\n",
    "\n",
    "algorithms_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e Avaliando seu próprio modelo\n",
    "\n",
    "Primeiramente, agora você deve decidir em qual ambiente você deseja treinar seu agente. Para isto, basta tirar o comentário da linha referente ao ambiente escolhido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"turing_envs:pong-normal-v0\"\n",
    "# env_name = \"LunarLander-v2\"\n",
    "# env_name = \"LunarLanderContinuous-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, você está livre para testar diferentes algoritmos para seu ambiente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import ... # Importe o modelo do stable_baselines3\n",
    "\n",
    "# Definindo o ambiente\n",
    "env = gym.make(env_name)\n",
    "\n",
    "model = ... # Defina o modelo\n",
    "model.learn(total_timesteps=...) # Treine o modelo\n",
    "n_episodes = ... # Defina o número de episódios\n",
    "\n",
    "# Avaliando o agente e guardando o desempenho no dicionário\n",
    "algorithms_dict = getValues(model, env, n_episodes, algorithms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço livre para testagem de diferentes algoritmos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, detalhe um pouco mais quais foram os algoritmos testados bem como a performance obtida por cada um.\n",
    "\n",
    "Este detalhamento pode ser feito por meio de um ou mais gráficos mostrando o desempenho dos modelos, ou simplesmente por texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Espaço para o Piloto criar gráficos ou textos para mostrar os diferentes resultados entre modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do Algoritmo\n",
    "\n",
    "Após testar e analisar diversos algoritmos diferentes, qual foi o escolhido?\n",
    "\n",
    "_Pergunta Extra:_ você usou algum critério para escolher quais algoritmos seriam testados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Qual foi o algoritmo escolhido?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c7f18c8a75fd108ead44a979ee6b583a0b7ac14f61cb5b6828c4aa8a7a81d42e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
