{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "![Imgur](https://i.imgur.com/ELdqWFi.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2 align=\"center\">üí∏ Falindo o Cassino de Monte Carlo üí∏</h2>\n",
    "\n",
    "Imagine que, ap√≥s uma bem sucedida corrida no *Circuito de M√¥naco*, voc√™ decidiu dar uma pausa na adrenalina do asfalto e passar em outra renomada atra√ß√£o tur√≠stica da cidade, o famoso **Cassino de Monte Carlo**.\n",
    "\n",
    "Por conta de sua excentricidade e lux√∫ria, os ca√ßa-n√≠queis (√†s vezes chamados de *one-armed bandits* em ingl√™s) desse cassino funcionavam de uma maneira diferente que a de costume: cada m√°quina possu√≠a 10 alavancas, com cada alavanca tendo uma certa chance de devolver uma certa quantidade de dinheiro. Como essas m√°quinas eram muito diferentes daquelas com que voc√™ estava acostumado, voc√™ decidiu observar um pouco alguns apostadores antes de participar do jogo. Com isso voc√™ observou que na m√©dia algumas alavancas devolviam mais dinheiro do que outras.\n",
    "\n",
    "Com sua ast√∫cia, voc√™ logo percebeu que poderia aplicar um algoritmo de aprendizado por refor√ßo para conseguir maximizar a quantidade de dinheiro recebida, possivelmente at√© falindo o cassino.\n",
    "\n",
    "![Imagem de um k-armed bandit](https://i.imgur.com/TxC4YwW.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Escrevendo a m√°quina ca√ßa-n√≠quel em c√≥digo\n",
    "\n",
    "A seguir vamos definir o que seria o ambiente (*enviroment* em ingl√™s) do nosso algoritmo. Nesse caso, ele √© a m√°quina ca√ßa-n√≠quel (vamos come√ßar a chamar de *bandit* daqui para frente), na qual voc√™ quer aprender qual √© a melhor alavanca a se puxar para receber a maior quantidade de dinheiro."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "\n",
    "    def __init__(self, k_arms=10):\n",
    "        # gera k n√∫meros aleat√≥rios uniformemente distribu√≠dos entre -3 e 3\n",
    "        self.bandits_expectations = np.random.uniform(-3, 3, k_arms)\n",
    "    \n",
    "    def gamble(self, action):\n",
    "        \"\"\"a√ß√£o(int) -> recompensa(int)\n",
    "        Recebe uma a√ß√£o representando a alavanca que ser√° acionada, \n",
    "        que devolve uma recompensa baseada em uma distribui√ß√£o normal \n",
    "        de m√©dia definida no init e desvio padr√£o 1.\n",
    "        \"\"\"\n",
    "        return np.random.normal(self.bandits_expectations[action], 1)"
   ]
  },
  {
   "source": [
    "Deixamos o c√≥digo em orienta√ß√£o a objetos porque √© o mais comum para ambientes em Python.\n",
    " - `__init__` √© o construtor do objeto. Nesse caso ele inicializa o objeto gerando *k* n√∫meros aleat√≥rios uniformemente distribu√≠dos entre -3 e 3. Cada um desses valores ser√° o valor esperado de retorno de cada alavanca.\n",
    " - `gamble` realiza uma das a√ß√µes poss√≠veis no ambiente e retorna a recompensa obtida. Para cada uma das *k* alavancas o agente pode escolher qual delas ir√° puxar. Por exemplo, se a a√ß√£o escolhida for o `0` ele puxar√° a alavanca de √≠ndice `0`, que possui valor esperado definido aleatoriamente pelo `__init__`.\n",
    "\n",
    "Vamos brincar um pouco com nosso ambiente para entender ele melhor:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31415)\n",
    "env = Bandit(5) # inicializa um bandit de 5 alavancas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ver os valores esperados de cada uma das alavancas\n",
    "for i in range(len(env.bandits_expectations)):\n",
    "  print(f\"Alavanca {i} tem valor esperado {env.bandits_expectations[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gamble(2) # puxar a alavanca de √≠ndice 2"
   ]
  },
  {
   "source": [
    "## Visualizando as distribui√ß√µes do Bandit\n",
    "Por conta de como est√£o distribu√≠dos os valores esperados, com certeza teremos alguma alavanca que possu√≠ o maior valor esperado. Podemos visualizar isso construindo um gr√°fico que mostra como os valores est√£o distribu√≠dos para cada alavanca:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = [[env.gamble(0) for i in range(1000)],\n",
    "           [env.gamble(1) for i in range(1000)],\n",
    "           [env.gamble(2) for i in range(1000)],\n",
    "           [env.gamble(3) for i in range(1000)],\n",
    "           [env.gamble(4) for i in range(1000)]]\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "\n",
    "max_e = np.max(env.bandits_expectations)\n",
    "\n",
    "plt.title(\"Distribui√ß√£o de cada alavanca de um bandit\")\n",
    "plt.violinplot(samples, [0,1,2,3,4])\n",
    "plt.axline((0,max_e), slope=0, ls=\"--\",c=\"red\", label=f\"Maior valor esperado = {max_e:.2f}\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"A√ß√£o\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.show()        "
   ]
  },
  {
   "source": [
    "Assim, com o gr√°fico podemos ver que realmente, a melhor alavanca a se puxar com o intuito de ganhar a maior quantidade de dinheiro √© a de n√∫mero 2. Por√©m, como podemos fazer com que o agente aprenda que essa √© a melhor alavanca a se puxar tamb√©m? Veremos isso na pr√≥xima se√ß√£o."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# ü•£ Criando um Algoritmo Guloso (*Greedy*)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Estimando  o valor esperado *Q*\n",
    "\n",
    "No nosso problema de $k$-Armed Bandits dizemos que para cada $k$ a√ß√µes h√° uma recompensa m√©dia esperada; esse valor esperado geralmente √© chamado de valor da a√ß√£o. Ou seja, definimos o valor de uma a√ß√£o arbitr√°ria $a$, denotado de $q^{*}(a)$, como uma recompensa em um tempo $t$ ($R_t$) dado que a a√ß√£o em $t$ ($A_t$) foi a como:\n",
    "\n",
    "$$q^*(a) = \\mathbb{E}[R_t | A_t=a]$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> (Esse $\\mathbb{E}$ significa o valor esperado, √© como se fosse a ‚Äúrecompensa m√©dia‚Äù ‚Äî a recompensa com maior probabilidade de acontecer dado $a$.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Se nosso agente soubesse todos os valores esperados, o problema seria facilmente resolvido: ele simplesmente escolheria a a√ß√£o com o maior valor. O problema √© justamente que ele n√£o sabe esses valores. Para obter alguma estimativa para esses valores, denotaremos as estimativas com a letra $Q$ mai√∫scula, j√° que normalmente em Aprendizado por Refor√ßo n√≥s usamos letras mai√∫sculas para representar algo aleat√≥rio ou um valor estimado.\n",
    "\n",
    "Como estamos buscando um valor esperado ‚Äî ou seja, a recompensa m√©dia ‚Äî basta calcularmos a m√©dia das recompensas recebidas por nosso agente naquela a√ß√£o:\n",
    "$$Q_{n+1} = \\frac{1}{n}\\sum_{i=1}^{n}R_i$$\n",
    "\n",
    "Por√©m, como na computa√ß√£o seria custoso executar uma somat√≥ria toda vez que gostar√≠amos de atualizar Q podemos fazer algumas manipula√ß√µes alg√©bricas e cair na seguinte equa√ß√£o:\n",
    "$$Q_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n) $$\n",
    "\n",
    "Que √© a equa√ß√£o que iremos usar em nosso algoritmo! Lembre-se que $n$, nesse caso, vai ser o n√∫mero de vezes que aquela a√ß√£o ocorreu. **Ent√£o teremos um $n$ e um $Q$ para cada a√ß√£o**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## A fun√ß√£o argmax\n",
    "\n",
    "Agora que j√° sabemos como estimar os valores $Q$ temos que criar alguma fun√ß√£o capaz de escolher a a√ß√£o com o maior valor estimado. Essa √© a fun√ß√£o argmax!\n",
    "\n",
    "Ela √© definida da seguinte maneira:\n",
    " - Recebe uma lista de valores\n",
    " - Verifica qual √© o maior valor dessa lista\n",
    " - Retorna o **√≠ndice** desse valor\n",
    " - Em caso de empates, ela retorna o √≠ndice de um dos maiores valores aleatoriamente\n",
    "\n",
    "![diagrama da argmax](https://i.imgur.com/B3HJC6P.png)\n",
    "\n",
    "Seu primeiro exerc√≠cio ser√° implementar essa fun√ß√£o!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exerc√≠cio 1, implementando a fun√ß√£o argmax\n",
    "Complete o c√≥digo abaixo para implementar a fun√ß√£o argmax especificada a cima."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(Q_values):\n",
    "    \"\"\" (lista) -> √≠ndice de maior valor(int)\n",
    "    Recebe uma lista dos valores Q e retorna o √≠ndice do maior valor.\n",
    "    Por defini√ß√£o, resolve empates escolhendo um deles aleatoriamente.\n",
    "    \"\"\"\n",
    "    max_value = float(\"-inf\")\n",
    "    ties = []\n",
    "\n",
    "    for i in range(len(Q_values)):\n",
    "        if ...:\n",
    "            pass\n",
    "        if ...:\n",
    "            pass\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "source": [
    "## Definindo o agente\n",
    "Agora que j√° sabemos como podemos estimar os valores de $Q$ e como podemos escolher as a√ß√µes de maior valor com a fun√ß√£o argmax, podemos definir nosso **agente guloso**. Ele dever√° ser composto da seguinte forma:\n",
    " - o construtor recebe o n√∫mero de `k_arms` existentes no ambiente e inicializa os seguintes atributos:\n",
    "    - `n_arms`: vetor que guardar√° quantas vezes o agente realizou cada a√ß√£o. Inicializado com um vetor de zeros com tamanho igual ao n√∫mero de a√ß√µes (alavancas).\n",
    "    - `Q_values`: vetor que armazena o $Q$ valor de cada a√ß√£o. Inicializado com um vetor de zeros com tamanho igual ao n√∫mero de a√ß√µes (alavancas).\n",
    "    -  `last_action`: √∫ltima a√ß√£o do agente. Inicializada com uma a√ß√£o aleat√≥ria.\n",
    "    \n",
    "- A fun√ß√£o de `step` deve receber a recompensa do epis√≥dio e estar definida da seguinte maneira:\n",
    "   - Atualiza o vetor `n_arms` adicionando mais um ao $n$ da √∫ltima a√ß√£o realizada.\n",
    "   - Calcula o $Q$ atual conforme a f√≥rmula definida.\n",
    "   - Atualiza o vetor `Q_values` atualizando o novo valor `Q` da √∫ltima a√ß√£o.\n",
    "   - Escolhe a pr√≥xima a√ß√£o passando a lista atualizada de `Q_values` √† fun√ß√£o argmax.\n",
    "   - Atualizar a √∫ltima a√ß√£o do agente com a a√ß√£o escolhida.\n",
    "   - Retorna a a√ß√£o escolhida."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exerc√≠cio 2, implementando o agente guloso\n",
    "Complete o c√≥digo abaixo conforme as defini√ß√µes:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent():\n",
    "    def __init__(self, k_arms):\n",
    "        self.n_arms = np.zeros(k_arms)\n",
    "        self.Q_values = np.zeros(k_arms)\n",
    "        self.last_action = np.random.randint(0, k_arms)\n",
    "\n",
    "    def agent_step(self, reward):\n",
    "        \"\"\" (float) -> acao(int)\n",
    "        D√° um step para o Agente atualizando os valores Q.\n",
    "        Pega a recompensas do estado e retorna a a√ß√£o escolhida.\n",
    "        \"\"\" \n",
    "\n",
    "        self.n_arms[...] = ...\n",
    "        current_Q = ...\n",
    "        self.Q_values[...] = ...\n",
    "\n",
    "        current_action = ...\n",
    "        self.last_action = ...\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Treinando o Agente\n",
    "Com o agente e o ambiente definidos, podemos finalmente trein√°-lo!\n",
    "\n",
    "O c√≥digo de treinamento deve ser definido como:\n",
    " - Inicializamos nosso agente `agent`, com `k_arms = 10`\n",
    " - Inicializamos nosso ambiente `env`, com `k_arms = 10`\n",
    " - Para cada `step` no n√∫mero total de steps  `num_steps`\n",
    "    - Receber a recompensa do ambiente puxando a alavanca (`env.gamble`) representada pela √∫ltima a√ß√£o do agente (`agent.last_action`).\n",
    "    - Atualizar a estimativa de valor do agente (`agent.agent_step`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exerc√≠cio 3, implementando o treinamento do agente\n",
    "Complete o c√≥digo a baixo conforme as defini√ß√µes:\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1_000\n",
    "\n",
    "agent = ...\n",
    "env = ...\n",
    "\n",
    "score = [0]\n",
    "means = []\n",
    "max_score = np.max(env.bandits_expectations)\n",
    "\n",
    "for step in range(n_steps):\n",
    "    reward = ... # Tome a√ß√£o no ambiente\n",
    "\n",
    "    ... # Atualize a estimativa de valor do agente\n",
    "\n",
    "    score.append(score[-1] + reward)\n",
    "    means.append(score[-1]/(step+1))\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "plt.title(\"Recompensa de uma simula√ß√£o de um agente guloso\")\n",
    "plt.axline((0, max_score), slope=0, ls=\"--\", c=\"red\", label=\"Melhor poss√≠vel\")\n",
    "plt.plot(means, label=\"Guloso\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Se tudo deu certo voc√™ ver√° o gr√°fico de uma simula√ß√£o, com a recompensa comparada com a melhor poss√≠vel. Voc√™ ver√° que, provavelmente, os resultados n√£o s√£o os melhores, mas existe uma chance de que, por conta de sorte, ele consiga a melhor pontua√ß√£o. Para eliminar esse fator de sorte podemos executar a simula√ß√£o v√°rias vezes e pegar a m√©dia:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1_000\n",
    "n_simulations = 200\n",
    "total_means = []\n",
    "\n",
    "max_means = 0\n",
    "for simulation in range(n_simulations):\n",
    "\n",
    "    agent = ...\n",
    "    env = ...\n",
    "\n",
    "    score = [0]\n",
    "    means = []\n",
    "\n",
    "    max_means += np.max(env.bandits_expectations)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        reward = ... # Tome a√ß√£o no ambiente\n",
    "\n",
    "        ... # Atualize a estimativa de valor do agente\n",
    "\n",
    "        score.append(score[-1] + reward)\n",
    "        means.append(score[-1]/(step+1))\n",
    "\n",
    "    total_means.append(means)\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "plt.title(\"Recompensa m√©dia de um agente guloso\")\n",
    "plt.axline((0, max_means/n_simulations), slope=0, ls=\"--\", c=\"red\", label=\"Melhor poss√≠vel\")\n",
    "plt.plot(np.mean(total_means, axis=0), label=\"Guloso\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Recompensa M√©dia\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Por mais que o Algoritmo Guloso consiga chegar rapidamente a uma recompensa m√©dia relativamente grande, ele ficar√° preso a ela para sempre. Por n√£o praticar explora√ß√£o ele acabar√° n√£o conhecendo outras a√ß√µes que podem ser melhores e, na m√©dia, ficar√° preso a uma solu√ß√£o n√£o otimizada.\n",
    "\n",
    "A solu√ß√£o para isso √© o o **Algoritmo Œµ-Guloso** que possui uma chance aleat√≥ria de fazer uma a√ß√£o de **explora√ß√£o**!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# üß≠ Criando um Algoritmo Œµ-Guloso\n",
    "A ideia de um algoritmo √© adicionar um par√¢metro $\\varepsilon$, que controla a probabilidade do agente realizar uma a√ß√£o aleat√≥ria de explora√ß√£o. A seguir, discutimos um pouco mais sobre esses dois conceitos.\n",
    "\n",
    "## Explora√ß√£o e Explota√ß√£o\n",
    "\n",
    "Como no agente guloso n√≥s sempre pegamos a a√ß√£o de maior valor estimado at√© aquele momento, √© f√°cil dele cair em solu√ß√µes sub√≥timas. Para resolver isso, introduzimos a ideia de **explora√ß√£o** no algoritmo.\n",
    "\n",
    "A explora√ß√£o √© uma maneira do agente identificar cen√°rios ainda n√£o vistos, conhecendo melhor o valor de cada a√ß√£o. A explota√ß√£o, por sua vez, √© utilizar esse conhecimento para tomar a melhor decis√£o. Uma analogia com o mundo real seria o menu de um restaurante: imagine que voc√™ pediu um prato l√° e acabou gostando deste prato, voc√™ poderia sempre pedi-lo quando fosse nesse restaurante e acabaria feliz, por√©m, se n√£o se arriscar a pedir nenhum outro prato nunca saber√° se pode haver um prato do qual voc√™ acabe gostando mais!\n",
    "\n",
    "## Definindo o Agente\n",
    "No geral, o agente $\\varepsilon$-guloso √© bem parecido com o agente guloso, a maior diferen√ßa sendo a presen√ßa desse novo par√¢metro $\\varepsilon$ e a chance $\\varepsilon$ dele executar uma a√ß√£o aleat√≥ria ao inv√©s da de maior valor estimado.\n",
    "\n",
    "Com isso o c√≥digo ficaria definido como:\n",
    " - No construtor, al√©m do par√¢metro `k_arms`, agora ele deve receber o par√¢metro `epsilon`.\n",
    "    - Al√©m dos atributos j√° definidos no agente guloso, agora ele tamb√©m deve inicializar um atributo `self.epsilon` que armazena o valor de `epsilon`\n",
    " - Na fun√ß√£o de `step` todos os passos de atualiza√ß√£o dos vetores continuam os mesmos, o que muda √© agora colocar a chance de executar uma a√ß√£o aleat√≥ria\n",
    "    - Ao inv√©s de s√≥ escolher a pr√≥xima a√ß√£o com base no argmax voc√™ deve gerar um n√∫mero real aleat√≥rio entre 0 e 1.\n",
    "    - Se esse n√∫mero for menor que o epsilon, a pr√≥xima a√ß√£o deve ser aleat√≥ria (um n√∫mero inteiro aleat√≥rio entre 0 e n√∫mero de bra√ßos-1)\n",
    "    - Se n√£o, a pr√≥xima a√ß√£o √© a com maior $Q$ valor (passando a lista de valores $Q$ na argmax)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exerc√≠cio 4, implementado o agente Œµ-Guloso"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "    def __init__(self, k_arms, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_arms = np.zeros(k_arms)\n",
    "        self.Q_values = np.zeros(k_arms)\n",
    "        self.last_action = np.random.randint(0, k_arms)\n",
    "\n",
    "    def agent_step(self, reward):\n",
    "        \"\"\" (float) -> acao(int)\n",
    "        D√° um step para o Agente atualizando os valores Q.\n",
    "        Pega a recompensas do estado e retorna a a√ß√£o escolhida.\n",
    "        \"\"\" \n",
    "\n",
    "        self.n_arms[...] = ...\n",
    "        current_Q = ...\n",
    "        self.Q_values[...] = ...\n",
    "\n",
    "        u = ... # n√∫mero aleat√≥rio real entre 0 e 1\n",
    "\n",
    "        if ...:\n",
    "            current_action = ...\n",
    "        else:\n",
    "            current_action = ...\n",
    "\n",
    "        self.last_action = ...\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "source": [
    "## Treinando o Agente\n",
    "O processo de treinamento √© o mesmo, o √∫nico diferencial √© que agora podemos testar diferentes resultados para diferentes valores de $\\varepsilon$.\n",
    " - Lembre-se de inicializar o agente e o ambiente. Para o agente, lembre de passar o par√¢metro de `epsilon=eps`.\n",
    " - Dentro da lista `epsilons` escreva alguns valores para o par√¢metro. Recomendamos tr√™s em espec√≠fico: 0.1, 0.5 e 0 (se `epsilon = 0` ent√£o √© s√≥ o algoritmo guloso), mas sinta-se livre para experimentar mais par√¢metros.\n",
    " - Como foram os resultados obtidos? Qual √© a rela√ß√£o de mudar o valor de $\\varepsilon$ e a m√©dia das recompensas m√©dias?\n",
    " - Sinta-se livre para tamb√©m testar valores de $\\varepsilon$ bem pequenos (0.01 por exemplo) e tamb√©m aumentar o n√∫mero de steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3_000\n",
    "n_simulations = 20\n",
    "total_means = []\n",
    "max_means = []\n",
    "\n",
    "# coloque outros par√¢metros para epsilon!\n",
    "epsilons = [0, 0.5, 0.1, ..., ...]\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "\n",
    "# Fazer 20 simula√ß√µes, com 3.000 steps para cada epsilon eps\n",
    "for eps in epsilons: \n",
    "    max_mean = 0\n",
    "    for runs in range(n_simulations):\n",
    "\n",
    "        agent = ... # inicialize o agente com um epsilon diferente\n",
    "\n",
    "        env = ... \n",
    "\n",
    "        score = [0]\n",
    "        means = []\n",
    "\n",
    "        max_mean += np.max(env.bandits_expectations)\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            reward = ... # Tome a√ß√£o no ambiente\n",
    "\n",
    "            ... # Atualize a estimativa de valor do agente\n",
    "\n",
    "            score.append(score[-1] + reward)\n",
    "            means.append(score[-1]/(step+1))\n",
    "        total_means.append(means)\n",
    "\n",
    "    max_means.append(max_mean/n_simulations)\n",
    "    \n",
    "    plt.plot(np.mean(total_means, axis=0), label=f\"{eps}\")\n",
    "\n",
    "y = np.mean(max_means, 0)\n",
    "plt.axline((0, y), slope=0, ls=\"--\", c=\"red\", label=\"Melhor poss√≠vel\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Recompensa M√©dia de um Agente Epsilon-Guloso\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Recompensa M√©dia\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Parab√©ns piloto! Se tudo deu certo voc√™ conseguir√° ver os resultados do seu modelo, que mostram como essa simples mudan√ßa consegue, ap√≥s um certo n√∫mero de itera√ß√µes, melhores resultados que um mero agente guloso!\n",
    "\n",
    "Agora, com seu modelo criado, voc√™ est√° pronto para faturar milh√µes no cassino de Monte Carlo, para depois gastar todo esse dinheiro em ***üç¶ sorvetinhos üç¶ ***, obviamente.\n",
    "\n",
    "> Caso esteja interessado, voc√™ pode olhar tamb√©m mais duas implementa√ß√µes que obt√©m resultados ainda melhores!\n",
    " - Bandits com [fun√ß√£o softmax](https://github.com/turing-usp/Aprendizado-por-Reforco/tree/main/Aprendizado%20por%20Refor%C3%A7o%20Cl%C3%A1ssico/Bandits/SoftMax) que calculam um valor de *prefer√™ncia* de uma a√ß√£o.\n",
    " - Bandits com [limite de confian√ßa superior](https://github.com/turing-usp/Aprendizado-por-Reforco/tree/main/Aprendizado%20por%20Refor%C3%A7o%20Cl%C3%A1ssico/Bandits/Limite%20de%20Confian%C3%A7a%20Superior) que utiliza um par√¢metro de confian√ßa para explorar e explotar.\n",
    " - Para um formalismo maior, voc√™ tamb√©m pode ler o cap√≠tulo de Bandits no fant√°stico livro do [Sutton & Barton](https://web.archive.org/web/20210608124316/https://incompleteideas.net/book/RLbook2020.pdf) refer√™ncia principal em Aprendizado por Refor√ßo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}