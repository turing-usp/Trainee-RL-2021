{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Stable Baselines\n",
    "\n",
    "## Índice\n",
    "\n",
    "- [O que é Stable Baselines?](#O-que-é-Stable-Baselines?)\n",
    "- [Como usar Stable Baselines?](#Como-usar-Stable-Baselines?)\n",
    "  - [Importando o Gym](#Importando-o-Gym)\n",
    "  - [O que é um Ambiente?](#O-que-é-um-Ambiente?)\n",
    "  - [Como Funciona um Ambiente do Gym?](#Como-Funciona-um-Ambiente-do-Gym?)\n",
    "  - [Criando um Ambiente](#Criando-um-Ambiente)\n",
    "  - [Criando um Agente](#Criando-um-Agente)\n",
    "  - [Rodando um Episódio](#Rodando-um-Episódio)\n",
    "  - [Treinamento](#Treinamento)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Stable Baselines?\n",
    "\n",
    "A **[Stable Baselines](https://github.com/hill-a/stable-baselines)** é uma biblioteca de Aprendizado por Reforço que implementa diversos algoritmos de agentes de RL, além de várias funcionalidades úteis para o treinamento de um agente. Suas implementações são bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de reforço de alta qualidade.\n",
    "\n",
    "![Logo](https://github.com/hill-a/stable-baselines/raw/master/docs//_static/img/logo.png \"Logo da Stable Baselines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como usar Stable Baselines?\n",
    "\n",
    "Com Stable Baselines, o processo de criar e treinar um agente é bem simples. Entretanto, caso você não saiba muito de Aprendizado por Reforço, é primeiro preciso passar por alguns conhecimentos básicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o Gym\n",
    "\n",
    "O **[Gym](https://gym.openai.com/)** é uma biblioteca desenvolvida pela OpenAI que contém várias implementações prontas de ambientes de Aprendizado por Reforço. Ela é muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu próprio ambiente.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif\" alt=\"Exemplos de Ambientes do Gym\" class=\"inline\"/>\n",
    "<figcaption>Exemplo de Ambientes do Gym</figcaption>\n",
    "<br>\n",
    "\n",
    "Para se ter acesso a esses ambientes, basta importar o Gym da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como Funciona um Ambiente do Gym?\n",
    "\n",
    "Agora que você já sabe o que é um ambiente, é preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns métodos simples para facilitar a comunicação com eles:\n",
    "\n",
    "<br>\n",
    "\n",
    "| Método               | Funcionalidade                                          |\n",
    "| :------------------- |:------------------------------------------------------- |\n",
    "| reset()              | Inicializa o ambiente e recebe a observação inicial     |\n",
    "| step(action)         | Executa uma ação e recebe a observação e a recompensa   |\n",
    "| render()             | Renderiza o ambiente                                    |\n",
    "| close()              | Fecha o ambiente                                        |\n",
    "\n",
    "<br>\n",
    "\n",
    "Assim, o código para interagir com o ambiente costuma seguir o seguinte modelo:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "ambiente = gym.make(\"Nome do Ambiente\")                         # Cria o ambiente\n",
    "observação = ambiente.reset()                                   # Inicializa o ambiente\n",
    "acabou = False\n",
    "\n",
    "while not acabou:\n",
    "    ambiente.render()                                           # Renderiza o ambiente\n",
    "    observação, recompensa, acabou, info = ambiente.step(ação)  # Executa uma ação\n",
    "    \n",
    "ambiente.close()                                                # Fecha o ambiente\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Ambiente\n",
    "\n",
    "Para utilizar um dos ambientes do Gym, nós utilizamos a função ```gym.make()```, passando o nome do ambiente desejado como parâmetro e guardando seu valor retornado em uma variável que chamaramos de ```env```. A lista com todos os ambiente pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, nós vamos utilizar o ambiente ```CartPole-v1```, um ambiente bem simples que modela um pêndulo invertido em cima de um carrinho buscando seu estado de equilíbrio.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*jLj9SYWI7e6RElIsI3DFjg.gif\" width=\"400px\" alt=\"Ambiente do CartPole-v1\" class=\"inline\"/>\n",
    "\n",
    "#### CartPole\n",
    "\n",
    "Antes de treinar qualquer agente, primeiro é preciso entender melhor quais as características do nosso ambiente.\n",
    "\n",
    "O **Espaço de Observação** do CartPole é definido por 4 informações:\n",
    "\n",
    "<br>\n",
    "\n",
    "|     | Informação                         | Min     | Max    |\n",
    "| :-- | :--------------------------------- | :-----: | :----: |\n",
    "| 0   | Posição do Carrinho                | -4.8    | 4.8    |\n",
    "| 1   | Velocidade do Carrinho             | -Inf    | Inf    |\n",
    "| 2   | Ângulo da Barra                    | -24 deg | 24 deg |\n",
    "| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |\n",
    "\n",
    "<br>\n",
    "\n",
    "Dessa forma, a cada instante recebemos uma lista da observação com o seguinte formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.3661270e+00 -1.4129330e+38  2.4853730e-01  1.2598591e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já o **Espaço de Ação** é composto por duas ações únicas: mover o carrinho para a **esquerda** ou para a **direita**.\n",
    "\n",
    "Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos movê-lo para a direita, enviamos um `env.step(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Agente\n",
    "\n",
    "Depois de escolhermos nosso ambiente, já podemos pensar em qual algoritmo de agente queremos usar.\n",
    "\n",
    "A biblioteca disponibiliza algoritmos de diversos tipos, como *Deep Q-Networks* e *Actor-Critics*. Nem todos eles suportam todos os tipos de ambientes, então é recomendável dar uma olhada na [página oficial dos algoritmos](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html).\n",
    "\n",
    "#### Inicialização\n",
    "\n",
    "Todos os algoritmos são inicializados de uma forma parecida, nós instanciamos eles com alguns parâmetros em comum: ```policy```, que define a arquitetura da rede neural e ```env```, que define o ambiente no qual o agente vai treinar. Assim, a inicialização segue o seguinte formato:\n",
    "\n",
    "```python\n",
    "agente = ALGORITMO(policy, env)\n",
    "```\n",
    "\n",
    "Como exemplo, vamos criar um **PPO**, um tipo de Actor-Critic, para resolver o ambiente do **CartPole**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO('MlpPolicy', env, seed=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos o PPO com a `policy` 'MlpPolicy', que cria uma rede neural artificial com 2 camadas ocultas de 64 neurônios cada, no `env` do CartPole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os agentes também possuem alguns métodos em comuns bem importantes de se conhecer:\n",
    "\n",
    "<br>\n",
    "\n",
    "| Método        | Funcionalidade                          |\n",
    "| :------------ |:--------------------------------------- |\n",
    "| learn()       | Treina o agente                         |\n",
    "| predict(obs)  | Escolhe uma ação com base na observação |\n",
    "| save(caminho) | Salve o agente                          |\n",
    "| load(caminho) | Carrega o agente                        |\n",
    "\n",
    "<br>\n",
    "\n",
    "Dessa forma, se quisermos escolher a próxima ação do nosso agente, nós rodamos:\n",
    "\n",
    "```python\n",
    "agente.predict(observação)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodando um Episódio\n",
    "\n",
    "Bom, agora que já temos nosso agente e nosso ambiente, já podemos rodar nosso primeiro episódio!\n",
    "\n",
    "Para isso, vamos criar uma função `run_episode` para simplificar o processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# A função recebe o ambiente e o agente como parâmetros\n",
    "def run_episode(env, model, render=False):\n",
    "    # Primeiro, inicializamos o ambiente e guardamos a observação inicial em 'obs'\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Loop do episódio\n",
    "    for t in itertools.count():\n",
    "        # Nosso modelo prediz a ação 'action' a ser tomada com base na nossa observação 'obs'\n",
    "        action, _states = model.predict(obs)\n",
    "        \n",
    "        # Tomamos a ação 'action', e recebemos uma nova observação 'obs', uma recompensa 'reward'\n",
    "        # e se o episódio terminou 'done'\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Renderiza o ambiente, caso desejado\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        # Finaliza o episódio, caso tenha terminado\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Quando terminado, fechamos o ambiente\n",
    "    env.close()\n",
    "    \n",
    "    # Imprimindo a duração do ambiente\n",
    "    print(\"Duração do Episódio: \" + str(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, já podemos rodar o nosso agente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração do Episódio: 13\n"
     ]
    }
   ],
   "source": [
    "run_episode(env, model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente o resultado não foi tão bom assim. Isso é porque precisamos treinar nosso agente para que ele saiba as melhores ações a se tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "\n",
    "O treinamento do agente acontece de maneira bem simples, basta rodar o método `.learn()` com a quantidade de instantes de tempo `total_timesteps` que desejamos treinar.\n",
    "\n",
    "Ao longo do treinamento, nosso agente vai mostrando algumas informações importantes no ouput, como a função de custo `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 863  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 568         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008619683 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -4.02e+03   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.61        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 55          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 504          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071383463 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | -109         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    value_loss           | 38.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 429         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008124785 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | -7.72       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 59.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005741713 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | -5.35       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 67.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 327          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047061136 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | -1.17        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 53           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007546186 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | -0.0332     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 51          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011094484 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009602043 |\n",
      "|    clip_fraction        | 0.0156      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.02        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 31.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007621056 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.56        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 34.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 263         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009817034 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4           |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 22.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006793539 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00414    |\n",
      "|    value_loss           | 48          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009792595 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.54        |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.7         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00539     |\n",
      "|    value_loss           | 53.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x27a5b6eab48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já treinamos o nosso agente, podemos rodar mais um episódio para ver como ele melhorou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duração do Episódio: 500\n"
     ]
    }
   ],
   "source": [
    "run_episode(env, model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, você acabou de criar o seu primeiro agente com a Stable Baselines!\n",
    "\n",
    "Esse conhecimento já é suficiente para aplicar em vários ambientes simples, mas ainda existem várias outras funcionalidades muito interessantes da biblioteca que valem a pena aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD:\n",
    "\n",
    " - Ensinar a avaliar os modelos\n",
    "   - ```mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)```\n",
    " - Ensinar a fazer plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
