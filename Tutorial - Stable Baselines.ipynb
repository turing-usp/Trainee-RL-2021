{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Stable Baselines\r\n",
    "\r\n",
    "## Índice\r\n",
    "\r\n",
    "- [O que é Stable Baselines?](#O-que-é-Stable-Baselines?)\r\n",
    "- [Como usar Stable Baselines?](#Como-usar-Stable-Baselines?)\r\n",
    "  - [Importando o Gym](#Importando-o-Gym)\r\n",
    "  - [O que é um Ambiente?](#O-que-é-um-Ambiente?)\r\n",
    "  - [Como Funciona um Ambiente do Gym?](#Como-Funciona-um-Ambiente-do-Gym?)\r\n",
    "  - [Criando um Ambiente](#Criando-um-Ambiente)\r\n",
    "  - [Criando um Agente](#Criando-um-Agente)\r\n",
    "  - [Rodando um Episódio](#Rodando-um-Episódio)\r\n",
    "  - [Treinamento](#Treinamento)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é Stable Baselines?\n",
    "\n",
    "A **[Stable Baselines](https://github.com/hill-a/stable-baselines)** é uma biblioteca de Aprendizagem por Reforço que implementa diversos algoritmos de agentes de RL, além de várias funcionalidades úteis para o treinamento de um agente. Suas implementações são bem simples e intuitivas, mas sem deixarem de ser otimizadas e poderosas, buscando facilitar o desenvolvimento de projetos de reforço de alta qualidade.\n",
    "\n",
    "![Logo](https://github.com/hill-a/stable-baselines/raw/master/docs//_static/img/logo.png \"Logo da Stable Baselines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como usar Stable Baselines?\n",
    "\n",
    "Com Stable Baselines, o processo de criar e treinar um agente é bem simples. Entretanto, caso você não saiba muito de Aprendizagem por Reforço, é primeiro preciso passar por alguns conhecimentos básicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o Gym\n",
    "\n",
    "O **[Gym](https://gym.openai.com/)** é uma biblioteca desenvolvida pela OpenAI que contém várias implementações prontas de ambientes de Aprendizagem por Reforço. Ela é muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu próprio ambiente.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif\" alt=\"Exemplos de Ambientes do Gym\" class=\"inline\"/>\n",
    "<figcaption>Exemplo de Ambientes do Gym</figcaption>\n",
    "<br>\n",
    "\n",
    "Para se ter acesso a esses ambientes, basta importar o Gym da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como Funciona um Ambiente do Gym?\r\n",
    "\r\n",
    "Agora que você já sabe o que é um ambiente, é preciso entender como nosso agente interage efetivamente com ele. Todos os ambientes do Gym possuem alguns métodos simples para facilitar a comunicação com eles:\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "| Método               | Funcionalidade                                          |\r\n",
    "| :------------------- |:------------------------------------------------------- |\r\n",
    "| reset()              | Inicializa o ambiente e recebe a observação inicial     |\r\n",
    "| step(action)         | Executa uma ação e recebe a observação e a recompensa   |\r\n",
    "| render()             | Renderiza o ambiente                                    |\r\n",
    "| close()              | Fecha o ambiente                                        |\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "Assim, o código para interagir com o ambiente costuma seguir o seguinte modelo:\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "```python\r\n",
    "ambiente = gym.make(\"Nome do Ambiente\")                         # Cria o ambiente\r\n",
    "observação = ambiente.reset()                                   # Inicializa o ambiente\r\n",
    "acabou = False\r\n",
    "\r\n",
    "while not acabou:\r\n",
    "    ambiente.render()                                           # Renderiza o ambiente\r\n",
    "    observação, recompensa, acabou, info = ambiente.step(ação)  # Executa uma ação\r\n",
    "    \r\n",
    "ambiente.close()                                                # Fecha o ambiente\r\n",
    "```\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Ambiente\n",
    "\n",
    "Para utilizar um dos ambientes do Gym, nós utilizamos a função ```gym.make()```, passando o nome do ambiente desejado como parâmetro e guardando seu valor retornado em uma variável que chamaramos de ```env```. A lista com todos os ambiente pode ser encontrada [aqui](https://gym.openai.com/envs/#classic_control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, nós vamos utilizar o ambiente ```CartPole-v1```, um ambiente bem simples que modela um pêndulo invertido em cima de um carrinho buscando seu estado de equilíbrio.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*jLj9SYWI7e6RElIsI3DFjg.gif\" width=\"400px\" alt=\"Ambiente do CartPole-v1\" class=\"inline\"/>\n",
    "\n",
    "#### CartPole\n",
    "\n",
    "Antes de treinar qualquer agente, primeiro é preciso entender melhor quais as características do nosso ambiente.\n",
    "\n",
    "O **Espaço de Observação** do CartPole é definido por 4 informações:\n",
    "\n",
    "<br>\n",
    "\n",
    "|     | Informação                         | Min     | Max    |\n",
    "| :-- | :--------------------------------- | :-----: | :----: |\n",
    "| 0   | Posição do Carrinho                | -4.8    | 4.8    |\n",
    "| 1   | Velocidade do Carrinho             | -Inf    | Inf    |\n",
    "| 2   | Ângulo da Barra                    | -24 deg | 24 deg |\n",
    "| 3   | Velocidade na Extremidade da Barra | -Inf    | Inf    |\n",
    "\n",
    "<br>\n",
    "\n",
    "Dessa forma, a cada instante recebemos uma lista da observação com o seguinte formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.4226079e+00  1.6426810e+38  2.2506310e-01 -1.2768003e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já o **Espaço de Ação** é composto por duas ações únicas: mover o carrinho para a **esquerda** ou para a **direita**.\n",
    "\n",
    "Quando queremos mover o carrinho para a esquerda, fazemos um `env.step(0)`; quando queremos movê-lo para a direita, enviamos um `env.step(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um Agente\r\n",
    "\r\n",
    "Depois de escolhermos nosso ambiente, já podemos pensar em qual algoritmo de agente queremos usar.\r\n",
    "\r\n",
    "A biblioteca disponibiliza algoritmos de diversos tipos, como *Policy Gradients*, *Actor Critics*, *DQN*, etc. Nem todos eles suportam todos os tipos de ambientes, então é recomendável dar uma olhada na [página oficial dos algoritmos](https://stable-baselines.readthedocs.io/en/master/guide/algos.html).\r\n",
    "\r\n",
    "#### Inicialização\r\n",
    "\r\n",
    "Todos os algoritmos são inicializados de uma forma parecida, nós instanciamos eles com alguns parâmetros em comum: ```policy```, que define a arquitetura da rede neural e ```env```, que define o ambiente no qual o agente vai treinar. Assim, a inicialização segue o seguinte formato:\r\n",
    "\r\n",
    "```python\r\n",
    "agente = ALGORITMO(policy, env)\r\n",
    "```\r\n",
    "\r\n",
    "Como exemplo, vamos criar um **PPO**, um tipo de Actor-Critic, para resolver o ambiente do **CartPole**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E563CAEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E563CAEC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E563CAEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E563CAEC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E769B4988>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E769B4988>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E769B4988>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E769B4988>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E76E6CA88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E76E6CA88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E76E6CA88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000023E76E6CA88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\acer\\acer_simple.py:422: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\acer\\acer_simple.py:480: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines\\acer\\acer_simple.py:507: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO\r\n",
    "\r\n",
    "model = PPO('MlpPolicy', env, seed=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos o PPO com a `policy` 'MlpPolicy', que cria uma rede neural artificial com 2 camadas ocultas de 64 neurônios cada, no `env` do CartPole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os agentes também possuem alguns métodos em comuns bem importantes de se conhecer:\n",
    "\n",
    "<br>\n",
    "\n",
    "| Método        | Funcionalidade                          |\n",
    "| :------------ |:--------------------------------------- |\n",
    "| learn()       | Treina o agente                         |\n",
    "| predict(obs)  | Escolhe uma ação com base na observação |\n",
    "| save(caminho) | Salve o agente                          |\n",
    "| load(caminho) | Carrega o agente                        |\n",
    "\n",
    "<br>\n",
    "\n",
    "Dessa forma, se quisermos escolher a próxima ação do nosso agente, nós rodamos:\n",
    "\n",
    "```python\n",
    "agente.predict(observação)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodando um Episódio\n",
    "\n",
    "Bom, agora que já temos nosso agente e nosso ambiente, já podemos rodar nosso primeiro episódio!\n",
    "\n",
    "Para isso, vamos criar uma função `run_episode` para simplificar o processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# A função recebe o ambiente e o agente como parâmetros\n",
    "def run_episode(env, model, render=False):\n",
    "    # Primeiro, inicializamos o ambiente e guardamos a observação inicial em 'obs'\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Loop do episódio\n",
    "    for t in itertools.count():\n",
    "        # Nosso modelo prediz a ação 'action' a ser tomada com base na nossa observação 'obs'\n",
    "        action, _states = model.predict(obs)\n",
    "        \n",
    "        # Tomamos a ação 'action', e recebemos uma nova observação 'obs', uma recompensa 'reward'\n",
    "        # e se o episódio terminou 'done'\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Renderiza o ambiente, caso desejado\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        # Finaliza o episódio, caso tenha terminado\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Quando terminado, fechamos o ambiente\n",
    "    env.close()\n",
    "    \n",
    "    # Imprimindo a duração do ambiente\n",
    "    print(\"Duração: \" + str(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, já podemos rodar o nosso agente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "run_episode(env, model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente o resultado não foi tão bom assim. Isso é porque precisamos treinar nosso agente para que ele saiba as melhores ações a se tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "\n",
    "O treinamento do agente acontece de maneira bem simples, basta rodar o método `.learn()` com a quantidade de instantes de tempo `total_timesteps` que desejamos treinar.\n",
    "\n",
    "Ao longo do treinamento, nosso agente vai mostrando algumas informações importantes no ouput, como a duração média dos episódios `mean_episode_length`, a entropia `entropy`, o custo da função de custo `loss`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 19.7     |\n",
      "| avg_norm_grads_f    | 19.7     |\n",
      "| avg_norm_k          | 1.41     |\n",
      "| avg_norm_k_dot_g    | 19.7     |\n",
      "| entropy             | 14.6     |\n",
      "| explained_variance  | 0.000216 |\n",
      "| fps                 | 0        |\n",
      "| loss                | 37.8     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 6.84     |\n",
      "| loss_policy         | 6.84     |\n",
      "| loss_q              | 62.3     |\n",
      "| mean_episode_length | 0        |\n",
      "| mean_episode_reward | 0        |\n",
      "| norm_grads          | 4.02     |\n",
      "| norm_grads_policy   | 1.65     |\n",
      "| norm_grads_q        | 3.66     |\n",
      "| total_timesteps     | 20       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 17.4     |\n",
      "| avg_norm_g          | 39.1     |\n",
      "| avg_norm_grads_f    | 27.4     |\n",
      "| avg_norm_k          | 1.44     |\n",
      "| avg_norm_k_dot_g    | 42.6     |\n",
      "| entropy             | 10.1     |\n",
      "| explained_variance  | -0.011   |\n",
      "| fps                 | 96       |\n",
      "| loss                | 29.6     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | -6.8     |\n",
      "| loss_policy         | -6.8     |\n",
      "| loss_q              | 72.9     |\n",
      "| mean_episode_length | 46.8     |\n",
      "| mean_episode_reward | 46.8     |\n",
      "| norm_grads          | 20.5     |\n",
      "| norm_grads_policy   | 11.6     |\n",
      "| norm_grads_q        | 16.9     |\n",
      "| total_timesteps     | 5020     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 7.59     |\n",
      "| avg_norm_grads_f    | 7.59     |\n",
      "| avg_norm_k          | 1.43     |\n",
      "| avg_norm_k_dot_g    | 7.45     |\n",
      "| entropy             | 11.4     |\n",
      "| explained_variance  | -0.00919 |\n",
      "| fps                 | 91       |\n",
      "| loss                | 8.02     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 2.1      |\n",
      "| loss_policy         | 2.1      |\n",
      "| loss_q              | 12.1     |\n",
      "| mean_episode_length | 142      |\n",
      "| mean_episode_reward | 142      |\n",
      "| norm_grads          | 12.2     |\n",
      "| norm_grads_policy   | 1.05     |\n",
      "| norm_grads_q        | 12.2     |\n",
      "| total_timesteps     | 10020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 7.57     |\n",
      "| avg_norm_grads_f    | 7.57     |\n",
      "| avg_norm_k          | 1.5      |\n",
      "| avg_norm_k_dot_g    | 7.85     |\n",
      "| entropy             | 11.7     |\n",
      "| explained_variance  | 0.018    |\n",
      "| fps                 | 92       |\n",
      "| loss                | 7.46     |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 2.16     |\n",
      "| loss_policy         | 2.16     |\n",
      "| loss_q              | 10.8     |\n",
      "| mean_episode_length | 210      |\n",
      "| mean_episode_reward | 210      |\n",
      "| norm_grads          | 12       |\n",
      "| norm_grads_policy   | 1.45     |\n",
      "| norm_grads_q        | 11.9     |\n",
      "| total_timesteps     | 15020    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| avg_norm_adj        | 0        |\n",
      "| avg_norm_g          | 6.57     |\n",
      "| avg_norm_grads_f    | 6.57     |\n",
      "| avg_norm_k          | 1.42     |\n",
      "| avg_norm_k_dot_g    | 6.57     |\n",
      "| entropy             | 10.4     |\n",
      "| explained_variance  | 0.0153   |\n",
      "| fps                 | 98       |\n",
      "| loss                | 5.5      |\n",
      "| loss_bc             | -0       |\n",
      "| loss_f              | 1.78     |\n",
      "| loss_policy         | 1.78     |\n",
      "| loss_q              | 7.65     |\n",
      "| mean_episode_length | 233      |\n",
      "| mean_episode_reward | 233      |\n",
      "| norm_grads          | 11.4     |\n",
      "| norm_grads_policy   | 5.51     |\n",
      "| norm_grads_q        | 10       |\n",
      "| total_timesteps     | 20020    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.acer.acer_simple.ACER at 0x23e54fbc788>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20020, log_interval=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já treinamos o nosso agente, podemos rodar mais um episódio para ver como ele melhorou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "run_episode(env, model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, você acabou de criar o seu primeiro agente com a Stable Baselines!\n",
    "\n",
    "Esse conhecimento já é suficiente para aplicar em vários ambientes simples, mas ainda existem várias outras funcionalidades muito interessantes da biblioteca que valem a pena aprender."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}